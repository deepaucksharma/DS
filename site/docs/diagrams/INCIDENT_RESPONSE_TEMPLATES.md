# Incident Response Diagram Templates
## For When Production is on Fire at 3 AM

### ğŸš¨ Template 1: Service Down - Where to Look

```mermaid
graph TB
    subgraph "ğŸ”´ SERVICE DOWN - Debugging Path"
        Start[Service Unreachable] -->|1| CheckLB[Check Load Balancer<br/>â”â”â”â”â”<br/>ğŸ“Š AWS Console â†’ EC2 â†’ LB<br/>âœ“ Health checks failing?<br/>âœ“ Target groups healthy?]

        CheckLB -->|Unhealthy| CheckInstances[Check Instances<br/>â”â”â”â”â”<br/>ğŸ“Š kubectl get pods<br/>ğŸ“Š kubectl describe pod<br/>âœ“ OOMKilled?<br/>âœ“ CrashLoopBackOff?]

        CheckLB -->|Healthy| CheckApp[Check Application<br/>â”â”â”â”â”<br/>ğŸ“Š kubectl logs -f service-*<br/>âœ“ Panic/Exception?<br/>âœ“ Deadlock?<br/>âœ“ Resource exhaustion?]

        CheckInstances -->|OOM| MemoryFix[Increase Memory<br/>â”â”â”â”â”<br/>ğŸ”§ kubectl edit deployment<br/>ğŸ”§ resources.limits.memory: 4Gi<br/>ğŸ”§ kubectl rollout status]

        CheckInstances -->|Crash| LogAnalysis[Analyze Crash<br/>â”â”â”â”â”<br/>ğŸ“Š kubectl logs --previous<br/>ğŸ“Š Check error tracking<br/>ğŸ“Š Review recent deploys]

        CheckApp -->|Errors| CheckDeps[Check Dependencies<br/>â”â”â”â”â”<br/>ğŸ“Š Database connections?<br/>ğŸ“Š Redis available?<br/>ğŸ“Š External APIs?]

        CheckDeps -->|DB Issue| DBDebug[Database Debug<br/>â”â”â”â”â”<br/>ğŸ“Š SHOW PROCESSLIST<br/>ğŸ“Š Check slow query log<br/>ğŸ“Š Connection pool exhausted?]
    end

    style Start fill:#ff0000,color:#fff
    style CheckLB fill:#ff8800
    style CheckInstances fill:#ff8800
    style CheckApp fill:#ff8800
```

### ğŸ”¥ Template 2: Database is Slow

```mermaid
graph LR
    subgraph "ğŸŒ DATABASE SLOW - Investigation Flow"
        Symptom[High Latency<br/>â”â”â”â”â”<br/>âš ï¸ p99 > 1s<br/>âš ï¸ Timeouts] -->|1| Connections[Check Connections<br/>â”â”â”â”â”<br/>ğŸ“Š SHOW PROCESSLIST;<br/>ğŸ“Š Active: 450/500<br/>ğŸš¨ Pool exhausted?]

        Connections -->|2| SlowQueries[Find Slow Queries<br/>â”â”â”â”â”<br/>ğŸ“Š SHOW SLOW QUERIES;<br/>ğŸ“Š SELECT * FROM pg_stat_statements<br/>ğŸ“Š ORDER BY total_time DESC;]

        SlowQueries -->|3| ExplainPlan[Explain Plan<br/>â”â”â”â”â”<br/>ğŸ“Š EXPLAIN ANALYZE<br/>âœ“ Full table scan?<br/>âœ“ Missing index?<br/>âœ“ Bad join order?]

        ExplainPlan -->|4| QuickFix[Immediate Actions<br/>â”â”â”â”â”<br/>ğŸ”§ KILL long queries<br/>ğŸ”§ Add missing index<br/>ğŸ”§ Increase cache<br/>ğŸ”§ Scale read replicas]

        Connections -->|If Normal| Resources[Check Resources<br/>â”â”â”â”â”<br/>ğŸ“Š CPU: 95%<br/>ğŸ“Š Memory: 87%<br/>ğŸ“Š Disk I/O: 10K IOPS<br/>ğŸ“Š Network: Saturated?]

        Resources -->|5| Scale[Scaling Decision<br/>â”â”â”â”â”<br/>ğŸ”§ Vertical: r5.4xl â†’ r5.8xl<br/>ğŸ”§ Horizontal: Add replicas<br/>ğŸ”§ Caching: Add Redis]
    end

    style Symptom fill:#ff6600
    style QuickFix fill:#00aa00
    style Scale fill:#00aa00
```

### âš¡ Template 3: Latency Spike - Trace the Path

```mermaid
sequenceDiagram
    participant User
    participant CDN
    participant LB as Load Balancer
    participant API
    participant Cache
    participant DB

    Note over User,DB: ğŸ”´ LATENCY SPIKE - Where is the delay?

    User->>CDN: Request (Browser)<br/>â±ï¸ +5ms (normal)
    Note right of CDN: âœ… CDN OK<br/>Check: Cache hit rate

    CDN->>LB: Forward (miss)<br/>â±ï¸ +2ms (normal)
    Note right of LB: âœ… LB OK<br/>Check: Connection pool

    LB->>API: Route request<br/>â±ï¸ +500ms ğŸš¨ SLOW
    Note right of API: âŒ API SLOW<br/>Check: CPU, Memory<br/>Check: Thread pool<br/>Check: GC pauses

    API->>Cache: Get data<br/>â±ï¸ +1ms (normal)
    Note right of Cache: âœ… Cache OK<br/>Hit rate: 99%

    API->>DB: Query (cache miss)<br/>â±ï¸ +2000ms ğŸš¨ VERY SLOW
    Note right of DB: âŒ DB BOTTLENECK<br/>Check: Slow queries<br/>Check: Lock waits<br/>Check: Replication lag

    DB-->>API: Response<br/>â±ï¸ Total: 2508ms
    API-->>User: Response<br/>â±ï¸ Total: 2513ms

    Note over User,DB: ğŸ”§ Actions: Check API threads, DB slow query log
```

### ğŸ’¥ Template 4: Cascade Failure Pattern

```mermaid
graph TB
    subgraph "âš¡ CASCADE FAILURE - Timeline"
        T0[Initial Trigger<br/>â”â”â”â”â”<br/>12:00 - DB replica lag<br/>Lag: 5s â†’ 60s] -->|Slow reads| T1[Service Degradation<br/>â”â”â”â”â”<br/>12:02 - API timeouts<br/>Error rate: 0.1% â†’ 5%]

        T1 -->|Retries| T2[Load Amplification<br/>â”â”â”â”â”<br/>12:03 - Retry storm<br/>Request rate: 10K â†’ 50K/s]

        T2 -->|Overload| T3[Service A Fails<br/>â”â”â”â”â”<br/>12:05 - Circuit breaker open<br/>Availability: 0%]

        T3 -->|Dependency| T4[Service B Fails<br/>â”â”â”â”â”<br/>12:06 - Dependent on A<br/>Availability: 0%]

        T4 -->|Cascade| T5[Service C Fails<br/>â”â”â”â”â”<br/>12:07 - Dependent on B<br/>Availability: 0%]

        T5 -->|Platform Down| T6[Full Outage<br/>â”â”â”â”â”<br/>12:10 - All services affected<br/>Impact: 100% users]
    end

    subgraph "ğŸ”§ RECOVERY SEQUENCE"
        R1[Stop Incoming Traffic<br/>â”â”â”â”â”<br/>ğŸ”§ Enable maintenance mode]
        R2[Fix Root Cause<br/>â”â”â”â”â”<br/>ğŸ”§ Restart DB replica]
        R3[Reset Circuit Breakers<br/>â”â”â”â”â”<br/>ğŸ”§ Clear error counts]
        R4[Gradual Traffic Ramp<br/>â”â”â”â”â”<br/>ğŸ”§ 10% â†’ 25% â†’ 50% â†’ 100%]

        R1 --> R2 --> R3 --> R4
    end

    style T0 fill:#ffaa00
    style T3 fill:#ff0000
    style T6 fill:#cc0000
    style R1 fill:#00aa00
```

### ğŸ”„ Template 5: Circuit Breaker States

```mermaid
stateDiagram-v2
    [*] --> Closed: Start

    Closed --> Open: Failure Threshold<br/>â”â”â”â”â”<br/>Errors > 50%<br/>in 10 seconds

    Open --> HalfOpen: Timeout<br/>â”â”â”â”â”<br/>After 30 seconds

    HalfOpen --> Closed: Success<br/>â”â”â”â”â”<br/>Test request OK

    HalfOpen --> Open: Failure<br/>â”â”â”â”â”<br/>Test failed

    state Closed {
        [*] --> Monitoring
        Monitoring --> Monitoring: Success
        Monitoring --> Counting: Error
        Counting --> Monitoring: Error < 50%
        Counting --> [*]: Error >= 50%
    }

    state Open {
        [*] --> FastFailing
        FastFailing --> FastFailing: Reject all<br/>Return cached/default
        FastFailing --> [*]: Timer expires
    }

    state HalfOpen {
        [*] --> Testing
        Testing --> [*]: Single request
    }

    note right of Closed
        ğŸ“Š Metrics to Monitor:
        - Success rate
        - Response time
        - Error types
    end note

    note right of Open
        ğŸ”§ During Open State:
        - Serve from cache
        - Return defaults
        - Queue for retry
    end note
```

### ğŸ“Š Template 6: Memory Leak Detection

```mermaid
graph LR
    subgraph "ğŸ’¾ MEMORY LEAK - Investigation"
        Start[OOM Kills<br/>â”â”â”â”â”<br/>Pods restarting<br/>Every 2-3 hours] --> Heap[Heap Dump<br/>â”â”â”â”â”<br/>ğŸ“Š jmap -dump:format=b,file=/tmp/heap.hprof PID<br/>ğŸ“Š kubectl cp pod:/tmp/heap.hprof ./heap.hprof]

        Heap --> Analyze[Analyze Heap<br/>â”â”â”â”â”<br/>ğŸ”§ Eclipse MAT<br/>ğŸ”§ jhat heap.hprof<br/>ğŸ“Š Dominator tree<br/>ğŸ“Š Leak suspects]

        Analyze --> Found[Found Leak<br/>â”â”â”â”â”<br/>ğŸ“ HashMap growing<br/>ğŸ“ Connection pool<br/>ğŸ“ Cache unbounded]

        Found --> Fix[Fix & Deploy<br/>â”â”â”â”â”<br/>ğŸ”§ Add size limit<br/>ğŸ”§ Implement LRU<br/>ğŸ”§ Close connections<br/>ğŸ”§ Deploy canary]

        Start --> Metrics[Memory Metrics<br/>â”â”â”â”â”<br/>ğŸ“Š kubectl top pods<br/>ğŸ“Š Grafana dashboard<br/>ğŸ“Š Linear growth?]

        Metrics --> Profile[Profiling<br/>â”â”â”â”â”<br/>ğŸ”§ async-profiler<br/>ğŸ”§ pprof (Go)<br/>ğŸ”§ memory_profiler (Python)]
    end

    style Start fill:#ff6600
    style Fix fill:#00aa00
```

### ğŸŒ Template 7: Multi-Region Failure

```mermaid
graph TB
    subgraph "US-East-1 ğŸ”´ DOWN"
        USE1_LB[Load Balancer<br/>â”â”â”â”â”<br/>âŒ Unhealthy]
        USE1_APP[Application<br/>â”â”â”â”â”<br/>âŒ No response]
        USE1_DB[(Database<br/>â”â”â”â”â”<br/>âŒ Unreachable)]
    end

    subgraph "US-West-2 âœ… FAILOVER"
        USW2_LB[Load Balancer<br/>â”â”â”â”â”<br/>âœ… Healthy<br/>ğŸ”§ Receiving traffic]
        USW2_APP[Application<br/>â”â”â”â”â”<br/>âœ… Running<br/>âš ï¸ 2x normal load]
        USW2_DB[(Database<br/>â”â”â”â”â”<br/>âœ… Promoted to primary<br/>âš ï¸ Replication lag)]
    end

    subgraph "ğŸ“‹ FAILOVER CHECKLIST"
        Step1[1. Confirm East is down<br/>â”â”â”â”â”<br/>â˜ Multiple availability zones<br/>â˜ Not just monitoring]
        Step2[2. Update DNS<br/>â”â”â”â”â”<br/>â˜ Route53 health check<br/>â˜ TTL considerations]
        Step3[3. Promote West DB<br/>â”â”â”â”â”<br/>â˜ Check replication lag<br/>â˜ Accept data loss?]
        Step4[4. Scale West<br/>â”â”â”â”â”<br/>â˜ Double capacity<br/>â˜ Monitor closely]
        Step5[5. Notify<br/>â”â”â”â”â”<br/>â˜ Status page<br/>â˜ Customer communication]

        Step1 --> Step2 --> Step3 --> Step4 --> Step5
    end

    style USE1_LB fill:#ff0000
    style USE1_APP fill:#ff0000
    style USE1_DB fill:#ff0000
    style USW2_LB fill:#00aa00
    style USW2_APP fill:#00aa00
```

### ğŸ” Template 8: Distributed Tracing Debug

```mermaid
sequenceDiagram
    participant Client
    participant Gateway
    participant ServiceA
    participant ServiceB
    participant Database

    Note over Client,Database: ğŸ” TRACE ID: 7f3a2b1c-9d8e-4f5a

    Client->>Gateway: GET /api/order/123<br/>â”â”â”â”â”<br/>TraceID: 7f3a2b1c<br/>SpanID: 0001<br/>â±ï¸ 0ms

    Gateway->>ServiceA: CheckInventory()<br/>â”â”â”â”â”<br/>TraceID: 7f3a2b1c<br/>SpanID: 0002<br/>ParentSpan: 0001<br/>â±ï¸ 10ms

    ServiceA->>ServiceB: GetPricing()<br/>â”â”â”â”â”<br/>TraceID: 7f3a2b1c<br/>SpanID: 0003<br/>ParentSpan: 0002<br/>â±ï¸ 15ms

    ServiceB->>Database: SELECT price<br/>â”â”â”â”â”<br/>TraceID: 7f3a2b1c<br/>SpanID: 0004<br/>ParentSpan: 0003<br/>â±ï¸ 20ms

    Database-->>ServiceB: Result<br/>â”â”â”â”â”<br/>â±ï¸ 520ms ğŸš¨ SLOW

    ServiceB-->>ServiceA: Price data<br/>â”â”â”â”â”<br/>â±ï¸ 525ms

    ServiceA-->>Gateway: Inventory OK<br/>â”â”â”â”â”<br/>â±ï¸ 530ms

    Gateway-->>Client: Response<br/>â”â”â”â”â”<br/>â±ï¸ 535ms total

    Note over Client,Database: ğŸ”§ Finding: Database query slow (500ms)<br/>ğŸ”§ Action: Check query plan, add index
```

---

## ğŸ¯ Usage Guidelines

### When to Use Each Template

| Template | Scenario | Key Metrics |
|----------|----------|-------------|
| Service Down | Complete outage | Health checks, pod status |
| Database Slow | High latency | Query time, connections |
| Latency Spike | Performance degradation | p50, p99, p99.9 |
| Cascade Failure | Spreading outage | Error rates, dependencies |
| Circuit Breaker | Service protection | Failure %, state transitions |
| Memory Leak | OOM kills | Heap size, GC time |
| Multi-Region | DR scenario | RPO, RTO, data loss |
| Distributed Trace | Complex flows | Span timings, bottlenecks |

### Customization Required

For each template:
1. Replace generic names with actual service names
2. Add real metrics from your monitoring
3. Include specific commands for your environment
4. Link to your runbooks and dashboards

### Quality Checklist

- [ ] Shows clear debugging path
- [ ] Includes actual commands to run
- [ ] Has specific metrics to check
- [ ] Provides immediate actions
- [ ] References monitoring tools
- [ ] Helps at 3 AM when tired

---

## ğŸš¨ Remember

**These diagrams are for emergencies.**

They should be:
- **Actionable**: Every box has a command or check
- **Specific**: No generic "check database"
- **Sequential**: Clear order of investigation
- **Practical**: Based on real incidents

**If it doesn't reduce MTTR, it doesn't belong here.**