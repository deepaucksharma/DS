# CDN Capacity Planning Model\n\n## Executive Summary\n\n**Model Purpose**: Mathematical framework for sizing CDN infrastructure based on traffic patterns, geographic distribution, and performance requirements\n**Target Accuracy**: ±5% bandwidth prediction for global traffic patterns\n**Update Frequency**: Daily traffic analysis with weekly capacity adjustments\n**Confidence Level**: 99% based on real-time traffic monitoring\n\n## Real-World CDN Implementations\n\n### Netflix Open Connect (2023)\n- **Scale**: 15,000+ servers in 1,000+ locations worldwide\n- **Traffic**: 15+ petabytes daily, 200M+ concurrent streams\n- **Strategy**: Embedded CDN in ISP networks for optimal performance\n- **Cache Hit Rate**: 95%+ for popular content, <1ms latency\n- **Results**: 40% reduction in ISP transit costs, 99.9% availability\n\n### Cloudflare Edge Network (2022)\n- **Scale**: 275+ cities, 100+ countries, 50+ terabits capacity\n- **Performance**: 95% of global population within 50ms\n- **Architecture**: Anycast network with intelligent routing\n- **Cache Strategy**: Multi-tier caching with machine learning optimization\n- **Results**: <20ms p95 latency worldwide, 99.99% uptime\n\n### Amazon CloudFront (2023)\n- **Scale**: 450+ Points of Presence across 90+ cities\n- **Integration**: Deep AWS integration, edge computing capabilities\n- **Performance**: Sub-10ms latency for cached content\n- **Capacity Model**: Auto-scaling based on demand patterns\n- **Results**: 99.9% availability SLA, flexible pricing model\n\n## Core CDN Capacity Models\n\n### Traffic Distribution Model\n\n```mermaid\ngraph TB\n    subgraph \"Global CDN Traffic Distribution\"\n        subgraph \"Geographic Regions\"\n            AMERICAS[Americas<br/>40% of traffic<br/>Peak: 8PM EST<br/>Bandwidth: 100 Tbps]\n            \n            EMEA[EMEA<br/>35% of traffic<br/>Peak: 8PM CET<br/>Bandwidth: 85 Tbps]\n            \n            APAC[Asia-Pacific<br/>25% of traffic<br/>Peak: 8PM JST<br/>Bandwidth: 65 Tbps]\n        end\n        \n        subgraph \"Content Categories\"\n            VIDEO[Video Content<br/>70% of bandwidth<br/>Large files (1-10GB)<br/>High cache ratio: 90%]\n            \n            WEB[Web Assets<br/>20% of bandwidth<br/>Small files (<1MB)<br/>Medium cache ratio: 80%]\n            \n            API[API Responses<br/>10% of bandwidth<br/>Dynamic content<br/>Low cache ratio: 30%]\n        end\n        \n        subgraph \"Cache Hierarchy\"\n            EDGE[Edge Caches<br/>150+ locations<br/>SSD storage: 100TB each<br/>Cache duration: 24h]\n            \n            REGIONAL[Regional Caches<br/>25 locations<br/>Storage: 1PB each<br/>Cache duration: 7 days]\n            \n            ORIGIN[Origin Servers<br/>3 locations<br/>All content<br/>Permanent storage]\n        end\n    end\n\n    AMERICAS --> VIDEO\n    EMEA --> WEB\n    APAC --> API\n    \n    VIDEO --> EDGE\n    WEB --> REGIONAL\n    API --> ORIGIN\n    \n    EDGE --> REGIONAL\n    REGIONAL --> ORIGIN\n\n    %% Apply colors\n    classDef regionStyle fill:#51CF66,stroke:#00AA00,color:#fff\n    classDef contentStyle fill:#0066CC,stroke:#004499,color:#fff\n    classDef cacheStyle fill:#FF8800,stroke:#CC6600,color:#fff\n\n    class AMERICAS,EMEA,APAC regionStyle\n    class VIDEO,WEB,API contentStyle\n    class EDGE,REGIONAL,ORIGIN cacheStyle\n```\n\n### Bandwidth Calculation Model\n\n```\nRequired Bandwidth = Peak Concurrent Users × Average Bitrate × Cache Miss Ratio\n                   × Geographic Distribution Factor × Safety Margin\n\nWhere:\n- Peak Concurrent Users: Maximum simultaneous users per region\n- Average Bitrate: Weighted average across content types (Mbps)\n- Cache Miss Ratio: 1 - Cache Hit Rate (typically 0.05-0.30)\n- Geographic Distribution Factor: 1.2-2.0 (accounts for uneven distribution)\n- Safety Margin: 1.3-1.5 (30-50% headroom for spikes)\n```\n\n### Cache Sizing Mathematical Model\n\n```python\nimport math\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple\nfrom enum import Enum\n\nclass ContentType(Enum):\n    VIDEO = \"video\"\n    IMAGE = \"image\"\n    WEB_ASSET = \"web_asset\"\n    API_RESPONSE = \"api_response\"\n    SOFTWARE = \"software\"\n\nclass CacheLevel(Enum):\n    EDGE = \"edge\"\n    REGIONAL = \"regional\"\n    ORIGIN = \"origin\"\n\n@dataclass\nclass ContentCategory:\n    \"\"\"Content category specification for cache planning\"\"\"\n    content_type: ContentType\n    average_file_size_mb: float\n    requests_per_second: float\n    cache_hit_rate: float\n    cache_duration_hours: float\n    geographic_spread: float  # 0.0-1.0, how geographically distributed\n    \nclass CDNCapacityModel:\n    \"\"\"CDN capacity planning model\"\"\"\n    \n    def __init__(self):\n        self.content_categories: List[ContentCategory] = []\n        self.edge_locations: Dict[str, Dict] = {}\n        self.regional_locations: Dict[str, Dict] = {}\n    \n    def add_content_category(self, category: ContentCategory):\n        \"\"\"Add a content category to the capacity model\"\"\"\n        self.content_categories.append(category)\n    \n    def add_edge_location(self, location_id: str, \n                         expected_users: int, \n                         peak_multiplier: float = 3.0,\n                         distance_to_regional_km: int = 500):\n        \"\"\"Add an edge cache location\"\"\"\n        self.edge_locations[location_id] = {\n            \"expected_users\": expected_users,\n            \"peak_multiplier\": peak_multiplier,\n            \"distance_to_regional_km\": distance_to_regional_km,\n            \"peak_users\": int(expected_users * peak_multiplier)\n        }\n    \n    def add_regional_location(self, location_id: str,\n                            covered_edge_locations: List[str],\n                            backbone_capacity_gbps: float = 100.0):\n        \"\"\"Add a regional cache location\"\"\"\n        total_users = sum(\n            self.edge_locations[edge][\"peak_users\"] \n            for edge in covered_edge_locations \n            if edge in self.edge_locations\n        )\n        \n        self.regional_locations[location_id] = {\n            \"covered_edges\": covered_edge_locations,\n            \"total_peak_users\": total_users,\n            \"backbone_capacity_gbps\": backbone_capacity_gbps\n        }\n    \n    def calculate_cache_storage_requirements(self, location_id: str, \n                                           cache_level: CacheLevel) -> Dict[str, float]:\n        \"\"\"Calculate storage requirements for a cache location\"\"\"\n        \n        if cache_level == CacheLevel.EDGE and location_id not in self.edge_locations:\n            raise ValueError(f\"Edge location {location_id} not found\")\n        elif cache_level == CacheLevel.REGIONAL and location_id not in self.regional_locations:\n            raise ValueError(f\"Regional location {location_id} not found\")\n        \n        # Get user base for this location\n        if cache_level == CacheLevel.EDGE:\n            peak_users = self.edge_locations[location_id][\"peak_users\"]\n        else:\n            peak_users = self.regional_locations[location_id][\"total_peak_users\"]\n        \n        total_storage_gb = 0\n        storage_breakdown = {}\n        \n        for category in self.content_categories:\n            # Calculate working set size for this content category\n            \n            # Requests per hour during peak\n            peak_rps = category.requests_per_second * peak_users * category.geographic_spread\n            requests_per_hour = peak_rps * 3600\n            \n            # Unique content requests (accounting for cache hit rate)\n            unique_requests_per_hour = requests_per_hour * (1 - category.cache_hit_rate)\n            \n            # Content that needs to be cached\n            if cache_level == CacheLevel.EDGE:\n                cache_duration_factor = min(category.cache_duration_hours, 24) / 24\n            elif cache_level == CacheLevel.REGIONAL:\n                cache_duration_factor = min(category.cache_duration_hours, 168) / 168  # 7 days\n            else:\n                cache_duration_factor = 1.0  # Origin has everything\n            \n            # Calculate storage needed for this category\n            working_set_requests = unique_requests_per_hour * cache_duration_factor\n            category_storage_gb = (working_set_requests * category.average_file_size_mb) / 1024\n            \n            # Apply Zipf distribution (80/20 rule) - popular content is cached more\n            if cache_level == CacheLevel.EDGE:\n                category_storage_gb *= 0.2  # Edge caches only the most popular 20%\n            elif cache_level == CacheLevel.REGIONAL:\n                category_storage_gb *= 0.6  # Regional caches 60% of content\n            \n            storage_breakdown[category.content_type.value] = category_storage_gb\n            total_storage_gb += category_storage_gb\n        \n        # Add overhead for metadata, logs, and safety margin\n        metadata_overhead = total_storage_gb * 0.05  # 5% for metadata\n        safety_margin = total_storage_gb * 0.30      # 30% safety margin\n        \n        total_with_overhead = total_storage_gb + metadata_overhead + safety_margin\n        \n        return {\n            \"content_storage_breakdown\": storage_breakdown,\n            \"base_storage_gb\": total_storage_gb,\n            \"metadata_overhead_gb\": metadata_overhead,\n            \"safety_margin_gb\": safety_margin,\n            \"total_required_storage_gb\": total_with_overhead,\n            \"recommended_storage_tb\": math.ceil(total_with_overhead / 1024),\n            \"storage_utilization_target\": 0.70  # Target 70% utilization\n        }\n    \n    def calculate_bandwidth_requirements(self, location_id: str, \n                                       cache_level: CacheLevel) -> Dict[str, float]:\n        \"\"\"Calculate bandwidth requirements for a cache location\"\"\"\n        \n        # Get user base\n        if cache_level == CacheLevel.EDGE:\n            peak_users = self.edge_locations[location_id][\"peak_users\"]\n        else:\n            peak_users = self.regional_locations[location_id][\"total_peak_users\"]\n        \n        # Calculate bandwidth for each content category\n        total_egress_mbps = 0\n        total_ingress_mbps = 0\n        bandwidth_breakdown = {}\n        \n        for category in self.content_categories:\n            # Egress bandwidth (serving users)\n            user_requests_per_second = category.requests_per_second * peak_users * category.geographic_spread\n            egress_mbps = (user_requests_per_second * category.average_file_size_mb * 8) / 1  # Convert to Mbps\n            \n            # Ingress bandwidth (cache misses from upstream)\n            cache_miss_rate = 1 - category.cache_hit_rate\n            if cache_level == CacheLevel.EDGE:\n                # Edge pulls from regional on cache miss\n                ingress_rps = user_requests_per_second * cache_miss_rate\n            elif cache_level == CacheLevel.REGIONAL:\n                # Regional pulls from origin on cache miss\n                # But serves multiple edge locations\n                downstream_edges = len([e for e, data in self.edge_locations.items() \n                                      if any(loc == location_id for loc in [location_id])])\n                ingress_rps = user_requests_per_second * cache_miss_rate * 0.3  # Aggregation effect\n            else:\n                ingress_rps = 0  # Origin doesn't pull from anywhere\n            \n            ingress_mbps = (ingress_rps * category.average_file_size_mb * 8) / 1\n            \n            bandwidth_breakdown[category.content_type.value] = {\n                \"egress_mbps\": egress_mbps,\n                \"ingress_mbps\": ingress_mbps\n            }\n            \n            total_egress_mbps += egress_mbps\n            total_ingress_mbps += ingress_mbps\n        \n        # Add overhead for protocol overhead, retransmissions, etc.\n        protocol_overhead = 1.15  # 15% overhead\n        safety_factor = 1.50     # 50% safety margin for bursts\n        \n        total_egress_with_overhead = total_egress_mbps * protocol_overhead * safety_factor\n        total_ingress_with_overhead = total_ingress_mbps * protocol_overhead * safety_factor\n        \n        return {\n            \"bandwidth_breakdown\": bandwidth_breakdown,\n            \"base_egress_mbps\": total_egress_mbps,\n            \"base_ingress_mbps\": total_ingress_mbps,\n            \"total_egress_required_mbps\": total_egress_with_overhead,\n            \"total_ingress_required_mbps\": total_ingress_with_overhead,\n            \"recommended_egress_gbps\": math.ceil(total_egress_with_overhead / 1000),\n            \"recommended_ingress_gbps\": math.ceil(total_ingress_with_overhead / 1000),\n            \"peak_to_average_ratio\": 3.0  # Typical peak-to-average for CDN traffic\n        }\n    \n    def calculate_cache_hit_optimization(self, target_hit_rate: float = 0.90) -> Dict[str, any]:\n        \"\"\"Calculate optimal cache configuration to achieve target hit rate\"\"\"\n        \n        optimization_strategies = []\n        \n        for category in self.content_categories:\n            current_hit_rate = category.cache_hit_rate\n            \n            if current_hit_rate < target_hit_rate:\n                hit_rate_gap = target_hit_rate - current_hit_rate\n                \n                # Strategies to improve hit rate\n                strategies = {\n                    \"increase_cache_duration\": {\n                        \"current_hours\": category.cache_duration_hours,\n                        \"recommended_hours\": category.cache_duration_hours * 1.5,\n                        \"expected_improvement\": hit_rate_gap * 0.3\n                    },\n                    \"preload_popular_content\": {\n                        \"description\": \"Preload top 20% content based on popularity\",\n                        \"expected_improvement\": hit_rate_gap * 0.4\n                    },\n                    \"intelligent_purging\": {\n                        \"description\": \"Use LRU + popularity-based cache eviction\",\n                        \"expected_improvement\": hit_rate_gap * 0.2\n                    },\n                    \"increase_cache_size\": {\n                        \"current_factor\": 1.0,\n                        \"recommended_factor\": 1.5,\n                        \"expected_improvement\": hit_rate_gap * 0.3\n                    }\n                }\n                \n                optimization_strategies.append({\n                    \"content_type\": category.content_type.value,\n                    \"current_hit_rate\": current_hit_rate,\n                    \"target_hit_rate\": target_hit_rate,\n                    \"gap\": hit_rate_gap,\n                    \"strategies\": strategies\n                })\n        \n        return {\n            \"target_hit_rate\": target_hit_rate,\n            \"optimization_strategies\": optimization_strategies,\n            \"estimated_bandwidth_savings\": self._calculate_bandwidth_savings(target_hit_rate),\n            \"estimated_cost_impact\": self._calculate_cost_impact(target_hit_rate)\n        }\n    \n    def _calculate_bandwidth_savings(self, target_hit_rate: float) -> Dict[str, float]:\n        \"\"\"Calculate bandwidth savings from improved hit rates\"\"\"\n        current_total_bandwidth = 0\n        improved_total_bandwidth = 0\n        \n        for category in self.content_categories:\n            category_bandwidth = category.requests_per_second * category.average_file_size_mb * 8\n            \n            # Current bandwidth with cache misses\n            current_miss_rate = 1 - category.cache_hit_rate\n            current_bandwidth = category_bandwidth * current_miss_rate\n            current_total_bandwidth += current_bandwidth\n            \n            # Improved bandwidth with target hit rate\n            improved_miss_rate = 1 - target_hit_rate\n            improved_bandwidth = category_bandwidth * improved_miss_rate\n            improved_total_bandwidth += improved_bandwidth\n        \n        savings_mbps = current_total_bandwidth - improved_total_bandwidth\n        savings_percentage = (savings_mbps / current_total_bandwidth) * 100 if current_total_bandwidth > 0 else 0\n        \n        return {\n            \"current_bandwidth_mbps\": current_total_bandwidth,\n            \"improved_bandwidth_mbps\": improved_total_bandwidth,\n            \"savings_mbps\": savings_mbps,\n            \"savings_percentage\": savings_percentage\n        }\n    \n    def _calculate_cost_impact(self, target_hit_rate: float) -> Dict[str, float]:\n        \"\"\"Calculate cost impact of cache optimization\"\"\"\n        \n        # Typical CDN costs (example pricing)\n        bandwidth_cost_per_gb = 0.085  # $0.085 per GB\n        storage_cost_per_tb_per_month = 50  # $50 per TB per month\n        \n        bandwidth_savings = self._calculate_bandwidth_savings(target_hit_rate)\n        \n        # Monthly bandwidth cost savings (assuming 30 days, peak traffic 8 hours/day)\n        monthly_gb_saved = (bandwidth_savings[\"savings_mbps\"] / 8 * 1024) * 8 * 30 / 1024  # Convert to GB\n        monthly_bandwidth_savings = monthly_gb_saved * bandwidth_cost_per_gb\n        \n        # Additional storage costs (rough estimate)\n        storage_increase_factor = target_hit_rate / 0.80  # Assume current 80% hit rate baseline\n        additional_storage_cost = storage_cost_per_tb_per_month * (storage_increase_factor - 1) * 10  # 10TB baseline\n        \n        net_monthly_savings = monthly_bandwidth_savings - additional_storage_cost\n        \n        return {\n            \"monthly_bandwidth_savings\": monthly_bandwidth_savings,\n            \"additional_storage_cost\": additional_storage_cost,\n            \"net_monthly_savings\": net_monthly_savings,\n            \"annual_savings\": net_monthly_savings * 12,\n            \"roi_break_even_months\": abs(additional_storage_cost / monthly_bandwidth_savings) if monthly_bandwidth_savings > 0 else float('inf')\n        }\n    \n    def generate_capacity_report(self) -> Dict[str, any]:\n        \"\"\"Generate comprehensive CDN capacity planning report\"\"\"\n        \n        edge_analysis = {}\n        regional_analysis = {}\n        \n        # Analyze each edge location\n        for location_id in self.edge_locations:\n            storage_req = self.calculate_cache_storage_requirements(location_id, CacheLevel.EDGE)\n            bandwidth_req = self.calculate_bandwidth_requirements(location_id, CacheLevel.EDGE)\n            \n            edge_analysis[location_id] = {\n                \"users\": self.edge_locations[location_id],\n                \"storage_requirements\": storage_req,\n                \"bandwidth_requirements\": bandwidth_req\n            }\n        \n        # Analyze each regional location\n        for location_id in self.regional_locations:\n            storage_req = self.calculate_cache_storage_requirements(location_id, CacheLevel.REGIONAL)\n            bandwidth_req = self.calculate_bandwidth_requirements(location_id, CacheLevel.REGIONAL)\n            \n            regional_analysis[location_id] = {\n                \"coverage\": self.regional_locations[location_id],\n                \"storage_requirements\": storage_req,\n                \"bandwidth_requirements\": bandwidth_req\n            }\n        \n        # Cache optimization analysis\n        optimization = self.calculate_cache_hit_optimization(0.90)\n        \n        # Calculate total requirements\n        total_edge_storage = sum(a[\"storage_requirements\"][\"total_required_storage_gb\"] for a in edge_analysis.values())\n        total_edge_bandwidth = sum(a[\"bandwidth_requirements\"][\"total_egress_required_mbps\"] for a in edge_analysis.values())\n        \n        total_regional_storage = sum(a[\"storage_requirements\"][\"total_required_storage_gb\"] for a in regional_analysis.values())\n        total_regional_bandwidth = sum(a[\"bandwidth_requirements\"][\"total_egress_required_mbps\"] for a in regional_analysis.values())\n        \n        return {\n            \"content_summary\": {\n                \"total_categories\": len(self.content_categories),\n                \"total_edge_locations\": len(self.edge_locations),\n                \"total_regional_locations\": len(self.regional_locations)\n            },\n            \"edge_location_analysis\": edge_analysis,\n            \"regional_location_analysis\": regional_analysis,\n            \"capacity_totals\": {\n                \"total_edge_storage_tb\": total_edge_storage / 1024,\n                \"total_regional_storage_tb\": total_regional_storage / 1024,\n                \"total_edge_bandwidth_gbps\": total_edge_bandwidth / 1000,\n                \"total_regional_bandwidth_gbps\": total_regional_bandwidth / 1000\n            },\n            \"optimization_recommendations\": optimization,\n            \"cost_estimate\": self._estimate_monthly_costs(total_edge_storage, total_regional_storage, \n                                                          total_edge_bandwidth, total_regional_bandwidth)\n        }\n    \n    def _estimate_monthly_costs(self, edge_storage_gb: float, regional_storage_gb: float,\n                               edge_bandwidth_mbps: float, regional_bandwidth_mbps: float) -> Dict[str, float]:\n        \"\"\"Estimate monthly CDN operational costs\"\"\"\n        \n        # Cost assumptions (example pricing)\n        storage_cost_per_tb_month = 50\n        bandwidth_cost_per_gb = 0.085\n        edge_server_cost_per_month = 500  # Per server\n        regional_server_cost_per_month = 2000  # Per server\n        \n        # Storage costs\n        edge_storage_cost = (edge_storage_gb / 1024) * storage_cost_per_tb_month\n        regional_storage_cost = (regional_storage_gb / 1024) * storage_cost_per_tb_month\n        \n        # Bandwidth costs (assuming 8 hours peak daily)\n        monthly_edge_gb = (edge_bandwidth_mbps / 8 * 1024) * 8 * 30 / 1024\n        monthly_regional_gb = (regional_bandwidth_mbps / 8 * 1024) * 8 * 30 / 1024\n        \n        edge_bandwidth_cost = monthly_edge_gb * bandwidth_cost_per_gb\n        regional_bandwidth_cost = monthly_regional_gb * bandwidth_cost_per_gb\n        \n        # Server costs (rough estimates)\n        edge_servers_needed = len(self.edge_locations)\n        regional_servers_needed = len(self.regional_locations)\n        \n        edge_server_costs = edge_servers_needed * edge_server_cost_per_month\n        regional_server_costs = regional_servers_needed * regional_server_cost_per_month\n        \n        total_monthly_cost = (\n            edge_storage_cost + regional_storage_cost +\n            edge_bandwidth_cost + regional_bandwidth_cost +\n            edge_server_costs + regional_server_costs\n        )\n        \n        return {\n            \"edge_storage_cost\": edge_storage_cost,\n            \"regional_storage_cost\": regional_storage_cost,\n            \"edge_bandwidth_cost\": edge_bandwidth_cost,\n            \"regional_bandwidth_cost\": regional_bandwidth_cost,\n            \"edge_server_costs\": edge_server_costs,\n            \"regional_server_costs\": regional_server_costs,\n            \"total_monthly_cost\": total_monthly_cost,\n            \"cost_per_gb_served\": total_monthly_cost / (monthly_edge_gb + monthly_regional_gb) if (monthly_edge_gb + monthly_regional_gb) > 0 else 0\n        }\n\n# Usage example\nif __name__ == \"__main__\":\n    # Create CDN capacity model\n    cdn_model = CDNCapacityModel()\n    \n    # Add content categories\n    cdn_model.add_content_category(ContentCategory(\n        content_type=ContentType.VIDEO,\n        average_file_size_mb=500.0,  # 500MB average video file\n        requests_per_second=0.1,     # 0.1 requests per user per second\n        cache_hit_rate=0.92,         # 92% cache hit rate\n        cache_duration_hours=48,     # Cache for 48 hours\n        geographic_spread=0.8        # 80% of users request this content\n    ))\n    \n    cdn_model.add_content_category(ContentCategory(\n        content_type=ContentType.WEB_ASSET,\n        average_file_size_mb=0.5,    # 500KB average web asset\n        requests_per_second=2.0,     # 2 requests per user per second\n        cache_hit_rate=0.85,         # 85% cache hit rate\n        cache_duration_hours=24,     # Cache for 24 hours\n        geographic_spread=1.0        # All users request web assets\n    ))\n    \n    cdn_model.add_content_category(ContentCategory(\n        content_type=ContentType.API_RESPONSE,\n        average_file_size_mb=0.001,  # 1KB average API response\n        requests_per_second=5.0,     # 5 API calls per user per second\n        cache_hit_rate=0.30,         # 30% cache hit rate (mostly dynamic)\n        cache_duration_hours=1,      # Cache for 1 hour\n        geographic_spread=1.0        # All users make API calls\n    ))\n    \n    # Add edge locations\n    cdn_model.add_edge_location(\"new-york\", 500000, 3.5, 300)    # 500K users, 3.5x peak\n    cdn_model.add_edge_location(\"los-angeles\", 400000, 3.0, 400)\n    cdn_model.add_edge_location(\"london\", 300000, 2.8, 200)\n    cdn_model.add_edge_location(\"tokyo\", 600000, 4.0, 500)\n    cdn_model.add_edge_location(\"sydney\", 150000, 2.5, 800)\n    \n    # Add regional locations\n    cdn_model.add_regional_location(\"us-east\", [\"new-york\"], 200.0)\n    cdn_model.add_regional_location(\"us-west\", [\"los-angeles\"], 150.0)\n    cdn_model.add_regional_location(\"europe\", [\"london\"], 120.0)\n    cdn_model.add_regional_location(\"asia\", [\"tokyo\", \"sydney\"], 300.0)\n    \n    # Generate comprehensive report\n    report = cdn_model.generate_capacity_report()\n    \n    print(\"CDN Capacity Planning Report\")\n    print(\"=\" * 50)\n    \n    # Content summary\n    summary = report[\"content_summary\"]\n    print(f\"\\nContent Summary:\")\n    print(f\"- Content categories: {summary['total_categories']}\")\n    print(f\"- Edge locations: {summary['total_edge_locations']}\")\n    print(f\"- Regional locations: {summary['total_regional_locations']}\")\n    \n    # Capacity totals\n    totals = report[\"capacity_totals\"]\n    print(f\"\\nCapacity Requirements:\")\n    print(f\"- Total edge storage: {totals['total_edge_storage_tb']:.1f} TB\")\n    print(f\"- Total regional storage: {totals['total_regional_storage_tb']:.1f} TB\")\n    print(f\"- Total edge bandwidth: {totals['total_edge_bandwidth_gbps']:.1f} Gbps\")\n    print(f\"- Total regional bandwidth: {totals['total_regional_bandwidth_gbps']:.1f} Gbps\")\n    \n    # Cost estimate\n    costs = report[\"cost_estimate\"]\n    print(f\"\\nCost Estimate:\")\n    print(f\"- Total monthly cost: ${costs['total_monthly_cost']:,.2f}\")\n    print(f\"- Cost per GB served: ${costs['cost_per_gb_served']:.4f}\")\n    print(f\"- Edge bandwidth cost: ${costs['edge_bandwidth_cost']:,.2f}\")\n    print(f\"- Regional bandwidth cost: ${costs['regional_bandwidth_cost']:,.2f}\")\n    \n    # Top edge locations by requirements\n    print(f\"\\nTop Edge Locations:\")\n    edge_locations = [(loc, data) for loc, data in report[\"edge_location_analysis\"].items()]\n    edge_locations.sort(key=lambda x: x[1][\"bandwidth_requirements\"][\"total_egress_required_mbps\"], reverse=True)\n    \n    for loc, data in edge_locations[:3]:\n        storage_gb = data[\"storage_requirements\"][\"total_required_storage_gb\"]\n        bandwidth_mbps = data[\"bandwidth_requirements\"][\"total_egress_required_mbps\"]\n        users = data[\"users\"][\"peak_users\"]\n        print(f\"- {loc}: {users:,} peak users, {storage_gb:.0f}GB storage, {bandwidth_mbps:.0f}Mbps bandwidth\")\n    \n    # Optimization recommendations\n    optimization = report[\"optimization_recommendations\"]\n    if optimization[\"optimization_strategies\"]:\n        print(f\"\\nOptimization Opportunities:\")\n        for strategy in optimization[\"optimization_strategies\"][:2]:  # Top 2\n            content_type = strategy[\"content_type\"]\n            current_rate = strategy[\"current_hit_rate\"] * 100\n            target_rate = strategy[\"target_hit_rate\"] * 100\n            print(f\"- {content_type}: Improve hit rate from {current_rate:.1f}% to {target_rate:.1f}%\")\n        \n        cost_impact = optimization[\"estimated_cost_impact\"]\n        print(f\"- Potential annual savings: ${cost_impact['annual_savings']:,.2f}\")\n        print(f\"- ROI break-even: {cost_impact['roi_break_even_months']:.1f} months\")\n```\n\n### CDN Performance Optimization Models\n\n```mermaid\ngraph TB\n    subgraph \"CDN Performance Optimization Framework\"\n        subgraph \"Cache Optimization\"\n            HIT_RATIO[Cache Hit Ratio<br/>Target: 90%+ for static content<br/>Video: 95%, Web assets: 85%<br/>API responses: 30%]\n            \n            TTL_OPTIMIZATION[TTL Optimization<br/>Video: 48-72 hours<br/>Images: 24-48 hours<br/>CSS/JS: 12-24 hours<br/>API: 1-6 hours]\n            \n            PREFETCH[Intelligent Prefetching<br/>Popular content prediction<br/>Geographic patterns<br/>Time-based trending]\n        end\n        \n        subgraph \"Network Optimization\"\n            ANYCAST[Anycast Routing<br/>Automatic failover<br/>Latency-based routing<br/>Load distribution]\n            \n            BGP_OPTIMIZATION[BGP Route Optimization<br/>ISP peering agreements<br/>Direct connections<br/>Traffic engineering]\n            \n            COMPRESSION[Content Compression<br/>Brotli for text: 20% better<br/>WebP for images: 25-35%<br/>Video transcoding on-the-fly]\n        end\n        \n        subgraph \"Protocol Optimization\"\n            HTTP2[HTTP/2 & HTTP/3<br/>Multiplexing connections<br/>Server push capability<br/>QUIC protocol benefits]\n            \n            TCP_OPTIMIZATION[TCP Optimization<br/>BBR congestion control<br/>Initial window tuning<br/>Keep-alive optimization]\n            \n            SSL_OPTIMIZATION[SSL/TLS Optimization<br/>Session resumption<br/>OCSP stapling<br/>Certificate optimization]\n        end\n        \n        subgraph \"Edge Computing\"\n            EDGE_FUNCTIONS[Edge Functions<br/>Lambda@Edge<br/>Cloudflare Workers<br/>Dynamic content generation]\n            \n            ESI[Edge Side Includes<br/>Dynamic page assembly<br/>Personalization at edge<br/>A/B testing]\n        end\n    end\n\n    HIT_RATIO --> ANYCAST\n    TTL_OPTIMIZATION --> BGP_OPTIMIZATION\n    PREFETCH --> COMPRESSION\n    \n    ANYCAST --> HTTP2\n    BGP_OPTIMIZATION --> TCP_OPTIMIZATION\n    COMPRESSION --> SSL_OPTIMIZATION\n    \n    HTTP2 --> EDGE_FUNCTIONS\n    TCP_OPTIMIZATION --> ESI\n\n    %% Apply colors\n    classDef cacheStyle fill:#51CF66,stroke:#00AA00,color:#fff\n    classDef networkStyle fill:#0066CC,stroke:#004499,color:#fff\n    classDef protocolStyle fill:#FF8800,stroke:#CC6600,color:#fff\n    classDef edgeStyle fill:#9966CC,stroke:#663399,color:#fff\n\n    class HIT_RATIO,TTL_OPTIMIZATION,PREFETCH cacheStyle\n    class ANYCAST,BGP_OPTIMIZATION,COMPRESSION networkStyle\n    class HTTP2,TCP_OPTIMIZATION,SSL_OPTIMIZATION protocolStyle\n    class EDGE_FUNCTIONS,ESI edgeStyle\n```\n\n## Global Load Balancing and Failover Models\n\n### Geographic Load Balancing Strategy\n\n```mermaid\ngraph TB\n    subgraph \"Global CDN Load Balancing Architecture\"\n        subgraph \"DNS Layer\"\n            GLOBAL_DNS[Global DNS<br/>GeoDNS routing<br/>Health check integration<br/>Failover automation]\n        end\n        \n        subgraph \"Americas Region\"\n            US_EAST[US East<br/>Primary: Virginia<br/>Secondary: Ohio<br/>Capacity: 50 Tbps]\n            \n            US_WEST[US West<br/>Primary: Oregon<br/>Secondary: N. California<br/>Capacity: 40 Tbps]\n            \n            LATAM[Latin America<br/>Primary: São Paulo<br/>Secondary: Buenos Aires<br/>Capacity: 15 Tbps]\n        end\n        \n        subgraph \"EMEA Region\"\n            EUROPE[Europe<br/>Primary: Frankfurt<br/>Secondary: London<br/>Capacity: 35 Tbps]\n            \n            MIDDLE_EAST[Middle East<br/>Primary: Dubai<br/>Secondary: Bahrain<br/>Capacity: 10 Tbps]\n            \n            AFRICA[Africa<br/>Primary: Cape Town<br/>Secondary: Lagos<br/>Capacity: 5 Tbps]\n        end\n        \n        subgraph \"APAC Region\"\n            ASIA_NORTH[North Asia<br/>Primary: Tokyo<br/>Secondary: Seoul<br/>Capacity: 30 Tbps]\n            \n            ASIA_SOUTH[Southeast Asia<br/>Primary: Singapore<br/>Secondary: Mumbai<br/>Capacity: 20 Tbps]\n            \n            OCEANIA[Oceania<br/>Primary: Sydney<br/>Secondary: Melbourne<br/>Capacity: 8 Tbps]\n        end\n        \n        subgraph \"Health Monitoring\"\n            HEALTH_CHECKS[Health Check System<br/>HTTP/HTTPS probes<br/>TCP connectivity<br/>Response time monitoring<br/>5-second intervals]\n            \n            FAILOVER_LOGIC[Failover Logic<br/>Threshold: 3 failed checks<br/>Automatic DNS update<br/>Traffic redistribution<br/>Recovery detection]\n        end\n    end\n\n    GLOBAL_DNS --> US_EAST\n    GLOBAL_DNS --> EUROPE\n    GLOBAL_DNS --> ASIA_NORTH\n    \n    HEALTH_CHECKS --> US_EAST\n    HEALTH_CHECKS --> EUROPE\n    HEALTH_CHECKS --> ASIA_NORTH\n    HEALTH_CHECKS --> FAILOVER_LOGIC\n    \n    FAILOVER_LOGIC --> GLOBAL_DNS\n\n    %% Apply colors\n    classDef dnsStyle fill:#FF6B6B,stroke:#CC0000,color:#fff\n    classDef americasStyle fill:#51CF66,stroke:#00AA00,color:#fff\n    classDef emeaStyle fill:#0066CC,stroke:#004499,color:#fff\n    classDef apacStyle fill:#FFE066,stroke:#CC9900,color:#000\n    classDef monitorStyle fill:#9966CC,stroke:#663399,color:#fff\n\n    class GLOBAL_DNS dnsStyle\n    class US_EAST,US_WEST,LATAM americasStyle\n    class EUROPE,MIDDLE_EAST,AFRICA emeaStyle\n    class ASIA_NORTH,ASIA_SOUTH,OCEANIA apacStyle\n    class HEALTH_CHECKS,FAILOVER_LOGIC monitorStyle\n```\n\n## Cost Optimization and Pricing Models\n\n### CDN Cost Structure Analysis\n\n| Cost Component | Typical % of Total | Optimization Opportunities |\n|----------------|-------------------|-----------------------------|\n| **Bandwidth** | 60-70% | Cache hit ratio improvement, compression |\n| **Storage** | 15-20% | Intelligent cache eviction, tiered storage |\n| **Compute** | 10-15% | Edge function optimization, efficient processing |\n| **Network Infrastructure** | 8-12% | Peering agreements, direct connections |\n| **Operations** | 3-7% | Automation, monitoring efficiency |\n\n### Pricing Model Comparison\n\n| CDN Provider | Bandwidth ($/GB) | Requests ($/10K) | Storage ($/GB/month) | Special Features |\n|--------------|------------------|------------------|---------------------|------------------|\n| **CloudFlare** | $0.045-0.20 | $0.50 | $0.018 | DDoS protection included |\n| **AWS CloudFront** | $0.085-0.17 | $0.60 | $0.023 | AWS integration, Lambda@Edge |\n| **Google Cloud CDN** | $0.08-0.23 | $0.40 | $0.020 | Google network, edge computing |\n| **Azure CDN** | $0.081-0.25 | $0.22 | $0.024 | Microsoft integration |\n| **Fastly** | $0.12-0.20 | $2.00 | $0.040 | Real-time analytics, edge compute |\n\n## Monitoring and Alerting Models\n\n### CDN Performance Monitoring Framework\n\n```mermaid\ngraph TB\n    subgraph \"CDN Monitoring and Alerting System\"\n        subgraph \"Real-time Metrics\"\n            LATENCY[Response Latency<br/>p50, p95, p99 percentiles<br/>Target: <50ms globally<br/>Alert: >100ms sustained]\n            \n            THROUGHPUT[Throughput Metrics<br/>Requests per second<br/>Bytes per second<br/>Geographic breakdown]\n            \n            AVAILABILITY[Availability Metrics<br/>Uptime percentage<br/>Error rate tracking<br/>Status code distribution]\n        end\n        \n        subgraph \"Cache Performance\"\n            HIT_RATE[Cache Hit Rate<br/>Overall and by content type<br/>Geographic variations<br/>Time-based patterns]\n            \n            CACHE_EFFICIENCY[Cache Efficiency<br/>Storage utilization<br/>Eviction patterns<br/>Popular content analysis]\n        end\n        \n        subgraph \"Business Metrics\"\n            COST_PER_GB[Cost per GB Served<br/>Bandwidth costs<br/>Storage costs<br/>Operational efficiency]\n            \n            USER_EXPERIENCE[User Experience<br/>Time to first byte<br/>Page load times<br/>Video buffering ratio]\n        end\n        \n        subgraph \"Alerting Thresholds\"\n            CRITICAL_ALERTS[Critical Alerts<br/>Availability <99%<br/>Latency >200ms p95<br/>Hit rate <70%]\n            \n            WARNING_ALERTS[Warning Alerts<br/>Latency >100ms p95<br/>Hit rate <80%<br/>Error rate >1%]\n            \n            CAPACITY_ALERTS[Capacity Alerts<br/>Storage >80% full<br/>Bandwidth >85% peak<br/>Connection limits]\n        end\n    end\n\n    LATENCY --> CRITICAL_ALERTS\n    THROUGHPUT --> WARNING_ALERTS\n    AVAILABILITY --> CRITICAL_ALERTS\n    \n    HIT_RATE --> WARNING_ALERTS\n    CACHE_EFFICIENCY --> CAPACITY_ALERTS\n    \n    COST_PER_GB --> WARNING_ALERTS\n    USER_EXPERIENCE --> CRITICAL_ALERTS\n\n    %% Apply colors\n    classDef metricsStyle fill:#51CF66,stroke:#00AA00,color:#fff\n    classDef cacheStyle fill:#0066CC,stroke:#004499,color:#fff\n    classDef businessStyle fill:#FF8800,stroke:#CC6600,color:#fff\n    classDef alertStyle fill:#FF6B6B,stroke:#CC0000,color:#fff\n\n    class LATENCY,THROUGHPUT,AVAILABILITY metricsStyle\n    class HIT_RATE,CACHE_EFFICIENCY cacheStyle\n    class COST_PER_GB,USER_EXPERIENCE businessStyle\n    class CRITICAL_ALERTS,WARNING_ALERTS,CAPACITY_ALERTS alertStyle\n```\n\n## Conclusion and Best Practices\n\n### CDN Capacity Planning Checklist\n\n- [ ] **Traffic Analysis**: Historical patterns, geographic distribution, content types\n- [ ] **Performance Requirements**: Latency targets, throughput needs, availability SLAs\n- [ ] **Cache Strategy**: Hit rate optimization, TTL configuration, content classification\n- [ ] **Geographic Coverage**: PoP placement, regional capacity, failover planning\n- [ ] **Storage Planning**: Cache size per location, content popularity analysis\n- [ ] **Bandwidth Planning**: Peak capacity, burst handling, growth projections\n- [ ] **Cost Optimization**: Pricing model comparison, cache efficiency improvements\n- [ ] **Monitoring Setup**: Performance metrics, business KPIs, alert thresholds\n- [ ] **Disaster Recovery**: Multi-region failover, health checks, recovery procedures\n\n### Key CDN Capacity Formulas\n\n1. **Required Bandwidth** = Peak Users × Average Bitrate × (1 - Cache Hit Rate) × Geographic Factor × Safety Margin\n2. **Cache Storage Size** = Popular Content Size × Cache Duration Factor × Redundancy Factor\n3. **Cache Hit Rate** = (Total Requests - Origin Requests) ÷ Total Requests × 100%\n4. **Geographic Latency** = Physical Distance ÷ Speed of Light × Network Path Factor\n5. **Cost per GB** = (Monthly Infrastructure Cost + Operational Cost) ÷ Monthly GB Served\n\n### Performance Targets by Content Type\n\n| Content Type | Cache Hit Rate | TTL | Latency Target | Compression |\n|--------------|----------------|-----|----------------|-------------|\n| **Video Streaming** | 95%+ | 48-72 hours | <20ms first byte | H.264/H.265 optimized |\n| **Images** | 90%+ | 24-48 hours | <50ms | WebP, AVIF formats |\n| **Web Assets (CSS/JS)** | 85%+ | 12-24 hours | <30ms | Brotli compression |\n| **API Responses** | 30-70% | 1-6 hours | <100ms | JSON minification |\n| **Software Downloads** | 80%+ | 7 days | <100ms | Delta compression |\n\nCDN capacity planning requires balancing performance, cost, and global reach while ensuring optimal user experience across diverse geographic regions and content types. Regular monitoring and optimization ensure efficient resource utilization and cost management.