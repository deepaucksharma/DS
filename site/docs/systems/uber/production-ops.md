# Uber Production Operations - The Ops View

## System Overview

This diagram shows Uber's complete production operations including uDeploy deployment system handling 10,000+ deployments/week, cell-based architecture, M3 observability platform, and comprehensive chaos engineering practices.

```mermaid
graph TB
    subgraph EdgePlane[Edge Plane - Blue #3B82F6]
        style EdgePlane fill:#3B82F6,stroke:#2563EB,color:#fff

        GlobalDNS[Global DNS<br/>â”â”â”â”â”<br/>Route53 + CloudFlare<br/>Health-based routing<br/>TTL: 60 seconds<br/>Failover: 30 seconds]

        TrafficSplitter[Traffic Splitter<br/>â”â”â”â”â”<br/>A/B testing platform<br/>Feature flag integration<br/>Canary deployments<br/>1% â†’ 10% â†’ 100%]

        HealthChecks[Edge Health Checks<br/>â”â”â”â”â”<br/>Deep health validation<br/>5-second intervals<br/>Multi-region probes<br/>Auto-traffic routing]
    end

    subgraph ServicePlane[Service Plane - Green #10B981]
        style ServicePlane fill:#10B981,stroke:#059669,color:#fff

        subgraph CellA[Cell A (US-West-2a)]
            ServiceA[Matching Service<br/>â”â”â”â”â”<br/>100 instances<br/>c5.24xlarge<br/>Kubernetes pods<br/>Version: v2.47.3]

            GatewayA[API Gateway<br/>â”â”â”â”â”<br/>Load balancer<br/>Rate limiting<br/>Circuit breakers]
        end

        subgraph CellB[Cell B (US-West-2b)]
            ServiceB[Matching Service<br/>â”â”â”â”â”<br/>100 instances<br/>c5.24xlarge<br/>Kubernetes pods<br/>Version: v2.47.2]

            GatewayB[API Gateway<br/>â”â”â”â”â”<br/>Load balancer<br/>Rate limiting<br/>Circuit breakers]
        end

        subgraph CellC[Cell C (US-East-1a)]
            ServiceC[Matching Service<br/>â”â”â”â”â”<br/>100 instances<br/>c5.24xlarge<br/>Kubernetes pods<br/>Version: v2.47.3]

            GatewayC[API Gateway<br/>â”â”â”â”â”<br/>Load balancer<br/>Rate limiting<br/>Circuit breakers]
        end
    end

    subgraph StatePlane[State Plane - Orange #F59E0B]
        style StatePlane fill:#F59E0B,stroke:#D97706,color:#fff

        ConfigService[Configuration Service<br/>â”â”â”â”â”<br/>Real-time config updates<br/>Feature flag management<br/>Circuit breaker settings<br/>A/B test parameters]

        SecretManager[Secret Manager<br/>â”â”â”â”â”<br/>HashiCorp Vault<br/>Auto-rotation<br/>Encrypted at rest<br/>Audit logging]

        ServiceRegistry[Service Registry<br/>â”â”â”â”â”<br/>Consul + etcd<br/>Health monitoring<br/>Service discovery<br/>Load balancer config]
    end

    subgraph ControlPlane[Control Plane - Red #8B5CF6]
        style ControlPlane fill:#8B5CF6,stroke:#7C3AED,color:#fff

        subgraph DeploymentPipeline[uDeploy - Deployment Pipeline]
            GitLab[GitLab CI/CD<br/>â”â”â”â”â”<br/>Source control<br/>Merge requests<br/>Automated testing<br/>Security scanning]

            BuildSystem[Build System<br/>â”â”â”â”â”<br/>Docker image builds<br/>Multi-arch support<br/>Vulnerability scanning<br/>Artifact registry]

            uDeploy[uDeploy Orchestrator<br/>â”â”â”â”â”<br/>ğŸ† UBER'S DEPLOYMENT SYSTEM<br/>10K+ deployments/week<br/>Cell-based rollouts<br/>Automated rollbacks]

            TestingSuite[Testing Suite<br/>â”â”â”â”â”<br/>Unit tests: 100K+<br/>Integration tests: 10K+<br/>Load tests: 1K+<br/>Chaos tests: 100+]
        end

        subgraph ObservabilityStack[Observability Stack]
            M3Stack[M3 Metrics Platform<br/>â”â”â”â”â”<br/>10M metrics/second<br/>Real-time dashboards<br/>Custom time-series DB<br/>Query federation]

            LoggingStack[Logging Platform<br/>â”â”â”â”â”<br/>ELK Stack + Kafka<br/>1TB logs/day<br/>Real-time indexing<br/>Log correlation]

            TracingStack[Distributed Tracing<br/>â”â”â”â”â”<br/>Jaeger + OpenTelemetry<br/>1B spans/day<br/>Request correlation<br/>Performance analysis]

            AlertManager[Alert Manager<br/>â”â”â”â”â”<br/>PagerDuty integration<br/>Escalation policies<br/>Alert correlation<br/>Noise reduction]
        end

        subgraph ChaosEngineering[Chaos Engineering]
            ChaosPlatform[Chaos Platform<br/>â”â”â”â”â”<br/>Automated chaos tests<br/>Failure injection<br/>Blast radius control<br/>Recovery validation]

            ChaosMonkey[Chaos Monkey<br/>â”â”â”â”â”<br/>Random instance termination<br/>Daily experiments<br/>Service resilience<br/>Auto-remediation]

            ChaosKong[Chaos Kong<br/>â”â”â”â”â”<br/>Availability zone failures<br/>Weekly experiments<br/>Regional failover<br/>Data consistency]
        end
    end

    %% Deployment Flow
    GitLab -->|"1. Code push<br/>Automated triggers<br/>Security scan"| BuildSystem
    BuildSystem -->|"2. Build & test<br/>Docker images<br/>Vulnerability scan"| TestingSuite
    TestingSuite -->|"3. Test results<br/>Quality gates<br/>Performance benchmarks"| uDeploy

    uDeploy -->|"4. Cell A deploy<br/>1% traffic<br/>Health monitoring"| ServiceA
    ServiceA -->|"5. Health validation<br/>Success metrics<br/>Error rates"| uDeploy
    uDeploy -->|"6. Cell B deploy<br/>10% traffic<br/>Performance validation"| ServiceB
    ServiceB -->|"7. Final validation<br/>Full rollout<br/>Production ready"| ServiceC

    %% Configuration Management
    ConfigService -->|"Real-time config<br/>Feature flags<br/>Circuit breakers"| ServiceA
    ConfigService -->|"Synchronized config<br/>Consistency checks"| ServiceB
    ConfigService -->|"Global config<br/>Regional overrides"| ServiceC

    %% Service Discovery
    ServiceA -->|"Service registration<br/>Health status<br/>Load metrics"| ServiceRegistry
    ServiceB -->|"Service registration<br/>Health status<br/>Load metrics"| ServiceRegistry
    ServiceC -->|"Service registration<br/>Health status<br/>Load metrics"| ServiceRegistry

    %% Traffic Management
    GlobalDNS -->|"Health-based routing<br/>Geographic distribution"| TrafficSplitter
    TrafficSplitter -->|"A/B traffic split<br/>Canary routing"| GatewayA
    TrafficSplitter -->|"Load balancing<br/>Failover routing"| GatewayB
    TrafficSplitter -->|"Regional traffic<br/>Latency optimization"| GatewayC

    %% Observability
    ServiceA -.->|"10K metrics/sec<br/>Custom dashboards<br/>Real-time alerts"| M3Stack
    ServiceB -.->|"Application logs<br/>Error tracking<br/>Performance logs"| LoggingStack
    ServiceC -.->|"Request traces<br/>Latency analysis<br/>Dependency mapping"| TracingStack

    M3Stack -.->|"Threshold alerts<br/>Anomaly detection<br/>SLO violations"| AlertManager
    LoggingStack -.->|"Error alerts<br/>Pattern matching<br/>Log correlation"| AlertManager
    TracingStack -.->|"Latency alerts<br/>Error rate spikes<br/>Dependency failures"| AlertManager

    %% Chaos Engineering
    ChaosPlatform -.->|"Scheduled chaos<br/>Failure injection<br/>Recovery testing"| ServiceA
    ChaosMonkey -.->|"Random failures<br/>Instance termination<br/>Network partitions"| ServiceB
    ChaosKong -.->|"Regional failures<br/>AZ outages<br/>Cross-region tests"| ServiceC

    %% Apply operational colors
    classDef deploymentStyle fill:#4CAF50,stroke:#2E7D32,color:#fff,font-weight:bold
    classDef observabilityStyle fill:#2196F3,stroke:#1565C0,color:#fff,font-weight:bold
    classDef chaosStyle fill:#FF5722,stroke:#D32F2F,color:#fff,font-weight:bold
    classDef configStyle fill:#9C27B0,stroke:#6A1B9A,color:#fff,font-weight:bold

    class GitLab,BuildSystem,uDeploy,TestingSuite deploymentStyle
    class M3Stack,LoggingStack,TracingStack,AlertManager observabilityStyle
    class ChaosPlatform,ChaosMonkey,ChaosKong chaosStyle
    class ConfigService,SecretManager,ServiceRegistry configStyle
```

## uDeploy: Uber's Deployment System

### Deployment Pipeline Overview
**Scale**: 10,000+ deployments per week across 4,000+ microservices

#### Pipeline Stages
1. **Source Control**: GitLab with branch protection and review requirements
2. **Build & Test**: Automated builds with comprehensive testing suites
3. **Security Scanning**: Vulnerability assessment and compliance checks
4. **Deployment Orchestration**: Cell-based rollouts with health validation
5. **Monitoring & Rollback**: Real-time health monitoring with automatic rollback

### Cell-Based Architecture

#### Cell Configuration
```yaml
# Cell Definition
apiVersion: uber.com/v1
kind: Cell
metadata:
  name: matching-service-us-west-2a
spec:
  region: us-west-2
  availability_zone: us-west-2a
  capacity:
    instances: 100
    instance_type: c5.24xlarge
  traffic_allocation: 33%  # 1/3 of regional traffic
  blast_radius: single_az
  rollout_strategy:
    initial_percent: 1
    increment_percent: 10
    wait_duration: 300s  # 5 minutes between increments
```

#### Deployment Strategy
```
Stage 1: Canary Cell (1% traffic)
â”œâ”€â”€ Deploy to single cell
â”œâ”€â”€ Monitor for 10 minutes
â”œâ”€â”€ Validate SLO compliance
â””â”€â”€ Auto-rollback if errors > 0.1%

Stage 2: Regional Rollout (10% traffic)
â”œâ”€â”€ Deploy to 10% of regional cells
â”œâ”€â”€ Monitor for 30 minutes
â”œâ”€â”€ Validate performance metrics
â””â”€â”€ Proceed if latency < SLO + 10%

Stage 3: Full Rollout (100% traffic)
â”œâ”€â”€ Deploy to all cells progressively
â”œâ”€â”€ Monitor continuously
â”œâ”€â”€ Maintain previous version for 24h
â””â”€â”€ Complete rollout confirmation
```

### Deployment Metrics & SLOs

#### Success Metrics
- **Deployment Success Rate**: 99.7% (target: 99.5%)
- **Mean Time to Deploy**: 18 minutes (target: 20 minutes)
- **Rollback Rate**: 2.3% (target: <5%)
- **Mean Time to Rollback**: 3 minutes (target: <5 minutes)

#### Deployment Frequency
```
Daily Deployments by Service Type:
â”œâ”€â”€ Critical Services: 2-3 deploys/day
â”œâ”€â”€ Core Services: 5-10 deploys/day
â”œâ”€â”€ Feature Services: 10-20 deploys/day
â””â”€â”€ Experimental Services: 20+ deploys/day

Weekly Deployment Distribution:
â”œâ”€â”€ Monday: 2,500 deployments (25%)
â”œâ”€â”€ Tuesday: 2,200 deployments (22%)
â”œâ”€â”€ Wednesday: 2,000 deployments (20%)
â”œâ”€â”€ Thursday: 1,800 deployments (18%)
â”œâ”€â”€ Friday: 1,500 deployments (15%)
â”œâ”€â”€ Weekend: 0 deployments (deployment freeze)
```

## Observability Platform

### M3 Metrics Platform
**Scale**: 10 million metrics per second, 1.3 billion time series

#### Metric Categories
```python
# Business Metrics
trip_requests_total{city="san_francisco", status="completed"}
driver_utilization_ratio{region="us_west", hour="rush"}
revenue_per_trip{product="uber_x", city="nyc"}

# System Metrics
http_requests_total{service="matching", endpoint="/find_drivers"}
cpu_utilization{instance="matching-001", cell="us-west-2a"}
memory_usage_bytes{service="location", pod="location-47f3d"}

# Business Intelligence
supply_demand_ratio{h3_index="8928308280fffff", time="rush_hour"}
eta_accuracy{prediction_window="5min", actual_range="2min"}
cancellation_rate{reason="driver_no_show", city="chicago"}
```

#### Dashboard Hierarchy
```
Executive Dashboards:
â”œâ”€â”€ Global Business Health
â”œâ”€â”€ Revenue & Growth Metrics
â”œâ”€â”€ Regional Performance
â””â”€â”€ Competitive Analysis

Engineering Dashboards:
â”œâ”€â”€ Service Health Overview
â”œâ”€â”€ Infrastructure Utilization
â”œâ”€â”€ Deployment Pipeline Status
â””â”€â”€ Incident Response Dashboard

Team Dashboards:
â”œâ”€â”€ Service-specific SLOs
â”œâ”€â”€ Feature Flag Performance
â”œâ”€â”€ A/B Test Results
â””â”€â”€ Cost Optimization Metrics
```

### Distributed Tracing
**Scale**: 1 billion spans per day across 4,000+ services

#### Trace Sampling Strategy
```go
// Intelligent Sampling Configuration
sampling_rules:
  - service: "matching"
    operation: "find_drivers"
    sample_rate: 0.1  # 10% sampling for high-volume operations

  - service: "payment"
    operation: "process_charge"
    sample_rate: 1.0  # 100% sampling for critical operations

  - service: "*"
    operation: "*"
    sample_rate: 0.01  # 1% default sampling

  - error_rate: true
    sample_rate: 1.0  # 100% sampling for errors
```

#### Trace Analysis Capabilities
- **Request Flow Visualization**: End-to-end request journey
- **Latency Breakdown**: Service-by-service timing analysis
- **Error Correlation**: Link errors to specific code paths
- **Dependency Mapping**: Service interaction visualization

### Logging Infrastructure
**Scale**: 1TB of logs per day, real-time processing

#### Log Processing Pipeline
```yaml
# Kafka Topic Configuration
topics:
  - name: application_logs
    partitions: 100
    replication_factor: 3
    retention: 7d

  - name: audit_logs
    partitions: 20
    replication_factor: 3
    retention: 90d

  - name: security_logs
    partitions: 10
    replication_factor: 3
    retention: 365d

# Elasticsearch Configuration
indices:
  - pattern: "application-logs-*"
    shards: 5
    replicas: 1
    refresh_interval: 5s

  - pattern: "audit-logs-*"
    shards: 2
    replicas: 2
    refresh_interval: 30s
```

## Alerting & Incident Response

### Alert Configuration
```yaml
# SLO-based Alerting
alerts:
  - name: MatchingServiceLatency
    expression: |
      histogram_quantile(0.99,
        rate(http_request_duration_seconds_bucket{service="matching"}[5m])
      ) > 2.0
    for: 2m
    severity: critical
    escalation:
      - pagerduty: matching-team
      - slack: "#matching-alerts"

  - name: TripCompletionRate
    expression: |
      (
        rate(trips_completed_total[5m]) /
        rate(trips_started_total[5m])
      ) < 0.95
    for: 5m
    severity: warning
    escalation:
      - slack: "#operations"
      - email: ops-team@uber.com
```

### On-Call Procedures

#### Escalation Ladder
```
L1: Service Team On-Call
â”œâ”€â”€ Response time: 5 minutes
â”œâ”€â”€ Escalation trigger: 15 minutes
â”œâ”€â”€ Responsibilities: Initial triage, basic remediation
â””â”€â”€ Escalation path: L2 if unable to resolve

L2: Senior Engineer On-Call
â”œâ”€â”€ Response time: 10 minutes
â”œâ”€â”€ Escalation trigger: 30 minutes
â”œâ”€â”€ Responsibilities: Complex debugging, architectural decisions
â””â”€â”€ Escalation path: L3 for critical incidents

L3: Engineering Manager/Principal
â”œâ”€â”€ Response time: 15 minutes
â”œâ”€â”€ Escalation trigger: 1 hour or revenue impact
â”œâ”€â”€ Responsibilities: Resource coordination, external communication
â””â”€â”€ War room activation for critical incidents
```

#### Incident Response Playbooks
```markdown
# P0 Incident Response (Revenue/Safety Impact)
1. War room activation (< 5 minutes)
2. Incident commander assignment
3. Communication lead designation
4. Executive notification (< 15 minutes)
5. External communication plan
6. Resolution and post-mortem

# P1 Incident Response (Service Degradation)
1. Team on-call response
2. Initial impact assessment
3. Mitigation attempts
4. Escalation if needed
5. Resolution tracking
6. Root cause analysis

# P2 Incident Response (Minor Issues)
1. Standard on-call response
2. Fix during business hours
3. Documentation update
4. Process improvement
```

## Chaos Engineering Program

### Chaos Experiments Schedule
```
Daily Experiments:
â”œâ”€â”€ Chaos Monkey: Random instance termination
â”œâ”€â”€ Network Monkey: Latency/packet loss injection
â”œâ”€â”€ CPU Monkey: Resource exhaustion simulation
â””â”€â”€ Memory Monkey: Memory pressure testing

Weekly Experiments:
â”œâ”€â”€ Chaos Kong: Availability zone failures
â”œâ”€â”€ Database Monkey: Connection pool exhaustion
â”œâ”€â”€ Cache Monkey: Redis cluster failures
â””â”€â”€ Load Monkey: Traffic spike simulation

Monthly Experiments:
â”œâ”€â”€ Region Monkey: Complete regional outages
â”œâ”€â”€ Security Monkey: Certificate expiration
â”œâ”€â”€ Compliance Monkey: Data residency violations
â””â”€â”€ Disaster Monkey: Multi-region failures
```

### Chaos Engineering Results
```
Reliability Improvements (2024):
â”œâ”€â”€ 90% of issues caught before production
â”œâ”€â”€ 60% reduction in MTTR (Mean Time to Recovery)
â”œâ”€â”€ 40% improvement in service resilience
â””â”€â”€ 95% confidence in disaster recovery procedures

Service Resilience Scores:
â”œâ”€â”€ Matching Service: 99.97% availability
â”œâ”€â”€ Location Service: 99.95% availability
â”œâ”€â”€ Payment Service: 99.99% availability
â””â”€â”€ Analytics Service: 99.90% availability
```

## Security & Compliance

### Security Operations
```yaml
# Security Scanning Pipeline
security_scans:
  - type: SAST
    tool: SonarQube
    trigger: every_commit
    block_deployment: critical_vulnerabilities

  - type: DAST
    tool: OWASP_ZAP
    trigger: pre_production
    block_deployment: high_vulnerabilities

  - type: dependency_scan
    tool: Snyk
    trigger: daily
    block_deployment: high_risk_dependencies

  - type: container_scan
    tool: Twistlock
    trigger: image_build
    block_deployment: critical_vulnerabilities
```

### Compliance Monitoring
```
Regulatory Compliance:
â”œâ”€â”€ SOX: Financial controls and audit trails
â”œâ”€â”€ PCI DSS: Payment card data protection
â”œâ”€â”€ GDPR: European data privacy regulations
â”œâ”€â”€ CCPA: California consumer privacy act
â””â”€â”€ SOC2: Security and availability controls

Compliance Metrics:
â”œâ”€â”€ Audit trail completeness: 99.98%
â”œâ”€â”€ Data retention compliance: 100%
â”œâ”€â”€ Access control violations: <0.01%
â”œâ”€â”€ Encryption coverage: 100%
â””â”€â”€ Backup recovery testing: Monthly
```

## Performance Benchmarks

### Deployment Performance
```
Deployment Speed Improvements (2024 vs 2020):
â”œâ”€â”€ Build time: 45 minutes â†’ 8 minutes (82% improvement)
â”œâ”€â”€ Test execution: 120 minutes â†’ 25 minutes (79% improvement)
â”œâ”€â”€ Deployment time: 60 minutes â†’ 18 minutes (70% improvement)
â””â”€â”€ Total pipeline: 225 minutes â†’ 51 minutes (77% improvement)

Reliability Improvements:
â”œâ”€â”€ Failed deployments: 8.5% â†’ 0.3% (96% improvement)
â”œâ”€â”€ Rollback frequency: 12% â†’ 2.3% (81% improvement)
â”œâ”€â”€ Recovery time: 25 minutes â†’ 3 minutes (88% improvement)
â””â”€â”€ False positive alerts: 15% â†’ 2% (87% improvement)
```

### Operational Efficiency
```
Automation Benefits:
â”œâ”€â”€ Manual deployments: 90% â†’ 5% (94% reduction)
â”œâ”€â”€ Manual incident response: 60% â†’ 15% (75% reduction)
â”œâ”€â”€ Configuration drift: 25% â†’ 2% (92% reduction)
â””â”€â”€ Security patch time: 7 days â†’ 4 hours (95% improvement)

Cost Optimization:
â”œâ”€â”€ Infrastructure utilization: 45% â†’ 78% (73% improvement)
â”œâ”€â”€ Over-provisioning waste: 35% â†’ 8% (77% reduction)
â”œâ”€â”€ Manual operation costs: $15M/year â†’ $3M/year (80% reduction)
â””â”€â”€ Incident response costs: $25M/year â†’ $8M/year (68% reduction)
```

## Future Operations Roadmap (2024-2026)

### AI-Driven Operations
- **Predictive Scaling**: ML models for capacity prediction
- **Automated Incident Response**: AI-powered root cause analysis
- **Intelligent Alerting**: Context-aware alert correlation
- **Self-Healing Systems**: Automated remediation capabilities

### Platform Evolution
- **GitOps Integration**: Declarative infrastructure management
- **Service Mesh Adoption**: Istio for advanced traffic management
- **Serverless Migration**: Function-as-a-Service for event processing
- **Edge Computing**: Distributed processing for reduced latency

## Sources & References

- [Uber Engineering - uDeploy: Deploying Code at Scale](https://eng.uber.com/udeploy-code-at-scale/)
- [M3: Uber's Open Source, Large-scale Metrics Platform](https://eng.uber.com/m3/)
- [Building Uber's Distributed Tracing Infrastructure](https://eng.uber.com/distributed-tracing/)
- [Chaos Engineering at Uber](https://eng.uber.com/chaos-engineering/)
- [Incident Response at Uber](https://eng.uber.com/incident-response/)
- [Building Reliable Infrastructure at Scale](https://eng.uber.com/reliable-infrastructure/)
- SREcon 2024 - "Operating at Uber Scale: Lessons from 5B Trips"
- KubeCon 2024 - "Cell-Based Architecture for Global-Scale Applications"

---

*Last Updated: September 2024*
*Data Source Confidence: A (Official Uber Engineering)*
*Diagram ID: CS-UBR-OPS-001*